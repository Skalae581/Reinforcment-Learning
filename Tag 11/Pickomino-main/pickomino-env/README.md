
---

# Pickomino Gymnasium Environment üêõüé≤

Ein **Gymnasium**-kompatibles Environment f√ºr das W√ºrfelspiel **Pickomino (Heckmeck am Bratwurmeck)** inklusive Beispiel-**Q-Learning**-Agent.
Ziel: Einen Agenten trainieren, der in diesem MDP optimale Entscheidungen trifft (Sorte w√§hlen / stoppen).

## Inhalte

* `pickomino_env/pickomino_gym_env.py` ‚Äì deine `PickominoEnv` Klasse
* `pickomino_env/__init__.py` ‚Äì **automatische Registrierung** des Environments als `Pickomino-v0`
* `pyproject.toml` ‚Äì Paket-Metadaten & Abh√§ngigkeiten
* `rl_pickomino_qlearning.py` ‚Äì tabellarischer Q-Learning Agent (Beispiel-Training)

---

## Installation (Entwicklungsmodus)

```bash
# 1) Optional: virtuelle Umgebung
python -m venv .venv
# Windows: .venv\Scripts\activate
# Linux/Mac: source .venv/bin/activate

# 2) Abh√§ngigkeiten & Paket install.
pip install -e .
```

> `-e` (editable) verlinkt dein Arbeitsverzeichnis ‚Äì √Ñnderungen am Code wirken sofort.

---

## Projektstruktur

```
pickomino-env/
‚îú‚îÄ pyproject.toml
‚îú‚îÄ README.md
‚îî‚îÄ pickomino_env/
   ‚îú‚îÄ __init__.py                 # registriert "Pickomino-v0" beim Import
   ‚îî‚îÄ pickomino_gym_env.py        # class PickominoEnv(gym.Env)
```

Optional: `rl_pickomino_qlearning.py` im Root (Trainingsskript).

---

## Environment verwenden

Die Registrierung passiert automatisch beim Import von `pickomino_env`.

```python
import gymnasium as gym
import pickomino_env  # ‚ö†Ô∏è wichtig: l√∂st die Registrierung aus

env = gym.make("Pickomino-v0", num_players=2)  # kwargs √ºberschreiben Defaults
obs, info = env.reset(seed=42)
print("Init ok. Beispiel-Observation:", obs)
```

### Beobachtungen & Aktionen (derzeitige API)

* **Observation**: `obs = (dice_collected, dice_rolled)`
  Beide sind L√§ngen-6-Vektoren (Index 0 = Wurm, 1..5 = Augen).
* **Action**: `(face, roll_again)`

  * `face ‚àà {0..5}` (0=Wurm) ‚Üí nimm alle geworfenen W√ºrfel dieser Sorte
  * `roll_again ‚àà {0,1}` ‚Üí 0 = **stoppen**, 1 = **weiter w√ºrfeln**

> Hinweis: In der gelieferten Env sind die `observation_space`-Deklarationen noch `Discrete(6)`. F√ºr algorithmische Stabilit√§t empfiehlt sich **`Box(shape=(6,), dtype=int)`** o. **`MultiDiscrete([9]*6)`**. Das Beispiel-Training codiert die Observation intern selbst, daher l√§uft es auch so.

---

## Q-Learning Training (Beispiel)

Starte das mitgelieferte Trainingsskript:

```bash
python rl_pickomino_qlearning.py
```

Das Skript:

* encodiert Zust√§nde `(dice_collected, dice_rolled)` als hashbare Schl√ºssel,
* nutzt **Aktionsmasken** (nur geworfene Gesichter + eine STOP-Aktion),
* trainiert mit Œµ-greedy, Œ±, Œ≥,
* speichert Checkpoints `qtable_pickomino.pkl` und `qtable_pickomino_final.pkl`,
* druckt periodisch Durchschnitts-Return & greedy-Evaluation.

---

## Regeln (Kurzfassung)

* 8 W√ºrfel: `1..5` & **Wurm** (W). Wurm z√§hlt **5** zur Summe.
* Du **musst** mind. **einen Wurm** sammeln und **Summe ‚â• 21**, um ein Pl√§ttchen zu nehmen.
* Beim Stoppen nimmst du das **h√∂chste offene Pl√§ttchen ‚â§ Summe** (oder stiehlst exakt passendes Top-Pl√§ttchen eines Gegners).
* **Misswurf** (kein neues Gesicht w√§hlbar oder Stop ohne Voraussetzungen): oberstes eigenes Pl√§ttchen zur√ºck, h√∂chstes offenes wird zus√§tzlich umgedreht.

---

## Typische Stolpersteine & Fixes

1. **`ValueError: list.remove(x): x not in list`**
   Ursache: `step_tiles()` versucht `tile_table.remove(sum)`.
   **Fix:** Nimm **max(\[t for t in tile\_table if t ‚â§ sum])** nur bei **Stop** (oder wenn **keine W√ºrfel** √ºbrig) *und* nur mit **mind. einem Wurm**.

2. **`legal_move` setzt `self.terminated`, gibt aber lokale Flags zur√ºck**
   Konsistent machen: **nur lokale** Variablen setzen und zur√ºckgeben **oder** explizit `return self.terminated, self.truncated`.

3. **Observation-Space passt nicht**
   F√ºr Clean-Gym:

   ```python
   from gymnasium import spaces
   self.observation_space = spaces.Dict({
       "dice_collected": spaces.Box(low=0, high=8, shape=(6,), dtype=np.int64),
       "dice_rolled":    spaces.Box(low=0, high=8, shape=(6,), dtype=np.int64),
       "player":         spaces.Discrete(num_players),
   })
   ```

4. **Stop-Aktion**
   Im Agenten als eigene diskrete Aktion modelliert (z. B. ID 12), die auf `(face=0, roll_again=0)` gemappt wird.

---

## Tests (Schnellcheck)

```python
import gymnasium as gym, pickomino_env
env = gym.make("Pickomino-v0")
obs, info = env.reset(seed=0)
for _ in range(5):
    action = (1, 1)   # nimm ‚Äû1er‚Äú, dann weiterw√ºrfeln
    obs, r, term, trunc, info = env.step(action)
    print("r=", r, "term=", term, "trunc=", trunc)
```

---

## Entwicklung

* Format: `ruff` / `black` empfohlen
* Lint: `pip install ruff black`
* Run: `ruff check . && black .`

---

## Lizenz

W√§hle eine Lizenz (z. B. MIT) und lege eine Datei `LICENSE` ab:

```
MIT License (c) 2025 Jarl,Robin, Tanja
```

---

## Danksagung

* Spielidee: **Heckmeck am Bratwurmeck**
* RL-Beispiel: tabellarisches Q-Learning (einfache Baseline; f√ºr gr√∂√üere Zustandsr√§ume DQN empfehlen)

---

**Viel Erfolg beim Trainieren!** 
