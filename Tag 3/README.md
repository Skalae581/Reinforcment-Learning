
# ðŸ§  Reinforcement Learning Woche 1 â€“ EinfÃ¼hrung

---

## ðŸ“˜ Tag 1: Grundkonzepte & Motivation

### Was ist Reinforcement Learning?

- Ein Lernverfahren, bei dem ein **Agent** durch Interaktion mit einer **Umgebung** lernt, **Handlungen** auszufÃ¼hren.
- Ziel: **Maximierung kumulierter Belohnung (Reward)**.

**Begriffe:**
- **Agent:** Entscheidet und handelt.
- **Umgebung (Environment):** Gibt ZustÃ¤nde und Belohnungen zurÃ¼ck.
- **Zustand (State):** Momentane Situation.
- **Aktion (Action):** Entscheidung des Agenten.
- **Reward:** RÃ¼ckmeldung vom System.
- **Policy (Ï€):** Strategie, welche Aktion in welchem Zustand.

---

## ðŸ“— Tag 2: Markov Decision Processes (MDP) & Value Functions

Ein **MDP** besteht aus:
- ZustÃ¤nden \(S\)
- Aktionen \(A\)
- Ãœbergangswahrscheinlichkeiten \(P(s'|s,a)\)
- Belohnungsfunktion \(R(s,a)\)
- Discount-Faktor \( \gamma \in [0,1] \)

### Value Functions
- **State-Value:**  
  \( V^{\pi}(s) = \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} \mid S_0 = s \right] \)
- **Action-Value:**  
  \( Q^{\pi}(s,a) = \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} \mid S_0 = s, A_0 = a \right] \)

---

## ðŸ“˜ Tag 3: Bellman-Gleichung & Dynamic Programming

### Bellman-Gleichung:
FÃ¼r eine gegebene Policy \( \pi \):
\[
V^{\pi}(s) = R(s, \pi(s)) + \gamma \sum_{s'} P(s' \mid s, \pi(s)) V^{\pi}(s')
\]

FÃ¼r die optimale Policy:
\[
V^*(s) = \max_a \left[ R(s,a) + \gamma \sum_{s'} P(s' \mid s,a)V^*(s') \right]
\]

### Policy Iteration:
1. **Policy Evaluation**
2. **Policy Improvement**
3. Wiederholen bis Konvergenz

---

## ðŸ“™ Tag 4: Exploration, Q-Learning, Off-/On-Policy

### Q-Learning (Off-Policy):
\[
Q(s,a) \leftarrow Q(s,a) + \alpha \left[ r + \gamma \max_{a'} Q(s',a') - Q(s,a) \right]
\]

### Exploration vs. Exploitation
- **Exploration:** Neue Aktionen testen
- **Exploitation:** Die beste bekannte Aktion wÃ¤hlen
- **Îµ-greedy:** 
  - mit Wahrscheinlichkeit Îµ: zufÃ¤llige Aktion
  - sonst: beste bekannte Aktion

### On-Policy vs. Off-Policy
- **On-Policy:** Lernen mit aktueller Policy (z.â€¯B. SARSA)
- **Off-Policy:** Lernen mit anderer Ziel-Policy (z.â€¯B. Q-Learning)

---

## ðŸ“— Tag 5: Anwendungen & Konvergenz

### Anwendungsfelder:
- Spiele (AlphaGo, Atari)
- Robotik
- Empfehlungssysteme
- Smart Grids

### Konvergenzbedingungen:
- \( \gamma < 1 \)
- Lernrate \( \alpha \to 0 \) mit der Zeit
- Alle ZustÃ¤nde oft genug besucht

---

## ðŸ› ï¸ Bonus: Bootstrapping

Beim Bootstrapping:
- WertschÃ¤tzung wird durch SchÃ¤tzung zukÃ¼nftiger Werte **verbessert**
- Typisch fÃ¼r **Temporal Difference (TD)** Methoden
- Z.â€¯B. in Q-Learning oder SARSA

---

## âœ… Ãœbungsvorschlag

Implementiere ein 4x4-GridWorld mit Policy Iteration. Erstelle eine Heatmap der Wertefunktion und zeichne die aktuelle Policy ein.

=======
# Tag 3 Experimente

